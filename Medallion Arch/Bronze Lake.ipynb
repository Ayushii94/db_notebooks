{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53e2c17b-2da5-4809-9113-5f625c71a30b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# mounting the GCS bucket\n",
    "bucket_name = \"msklspace\"\n",
    "mount_name = \"ayushi_ecom\"\n",
    "dbutils.fs.mount(\n",
    "  f\"gs://{bucket_name}\",\n",
    "  f\"/mnt/databricks/{mount_name}\",\n",
    "  extra_configs = {\"fs.gs.project.id\": \"mentorsko-1700569460412\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d060a453-9a61-480f-91e9-09d3931e1e7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- create bronze schema\n",
    "create schema bronze;\n",
    "use schema bronze;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60cfd395-f818-46d6-9014-0085ed55d735",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# creating a list of all the tables (csv files) in the mounted bucket\n",
    "csv_files = dbutils.fs.ls(f\"dbfs:/mnt/{mount_name}\")\n",
    "Tables = [file.name[:-4] for file in csv_files if file.name.endswith('.csv')]\n",
    "print(Tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f008d3d-05ea-489f-81bf-de2f610ba652",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# creating empty tables in the bronze layer for each csv file\n",
    "for table in Tables:\n",
    "    create_table_query = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {table} (\n",
    "        -- Define your schema here\n",
    "    )\n",
    "    \"\"\"\n",
    "    spark.sql(create_table_query)\n",
    "    \n",
    "    #copying data from csv files to the bronze tables    \n",
    "    copy_into_query = f\"\"\"\n",
    "    COPY INTO {table}\n",
    "    FROM 'dbfs:/mnt/{mount_name}/{table}.csv'\n",
    "    FORMAT_OPTIONS(\"header\" = \"true\", \"inferSchema\" = \"true\", \"mergeSchema\" = \"true\", \"timestampFormat\" = \"dd-MM-yyyy HH.mm\")\n",
    "    COPY_OPTIONS(\"mergeSchema\" = \"true\")\n",
    "    \"\"\"\n",
    "    spark.sql(copy_into_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f07d2d2-18f0-4656-ac54-97074d03edb4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# enabling change data feed for each bronze table\n",
    "for table in Tables:\n",
    "    sql = \"ALTER TABLE bronze.{} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\".format(table)\n",
    "    spark.sql(sql)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze Lake",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
